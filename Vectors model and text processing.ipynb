{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82e67704",
   "metadata": {},
   "source": [
    "## Vector Models and text processing \n",
    "### By Zaky Riyadi\n",
    "- To convert text into numbers\n",
    "\n",
    "- **Sentence**: a sequence of word that typically starts with capitalized worf and ends with punctuation.\n",
    "- **Token**: can be reffered to as words, punctuation or sub-units of words (Therefore sentence is sequence of tokens).\n",
    "- **Tokenization** is a way of separating a piece of text into smaller units called tokens. Tokens can be either words, characters, or subwords (n-gram characters) tokenization.\n",
    "- **Characters**: commonly reffered to as letters, punctuation and whitespace.\n",
    "- **Vocabulary**: the set of \"all\" words.\n",
    "- **Corpus**: A large collection of writings of a specific kind or on a specific subject.\n",
    "- **N-Gram**: \n",
    "    - Refers to N consecutive items (e.g: words, subwords, characters).\n",
    "    - Single (one) item: unigram, two item: bigram, three items: trigram.\n",
    "    - use-case: the word2vec, Markov models.\n",
    "- What is **vectors**?\n",
    "    - Vectors are how we will represent text numerically and the foundation of ML techniques.\n",
    "    - Conventional definition: A quantity that has both magnitude and direction.\n",
    "    - ML definition: An array of scalars.\n",
    "    - a vector as a list of numbers, and vector algebra as operations performed on the numbers in the list.\n",
    "    \n",
    "- **Bag of words (BOW)**\n",
    "    - text is sequential and the sequence of word gives the text meaning. Eg: when the words are randomized, it would change meaning or making it incomprehensible.\n",
    "    - Despite this, many NLP method does **not consider the word order**, and this we call **bag of words**.\n",
    "    - BOW is commonly used by Vector models and classic ML models.\n",
    "    - Commonly, probabilistic and DL model do not use BOW approach.\n",
    " \n",
    "- **Count Vectorizer** \n",
    "    - Simple approach to converting text into vectors using BOW approach.\n",
    "    - Practical Issues:\n",
    "        - Text is represented in computer as string, but the algorithm requires counting individual words. --> **Tokenization**\n",
    "        - **Tokenization**: Convert a string (containing multiple of words), into list of words (each of which are strings).\n",
    "        - how do we know which word corresponds to which position of vector --> **Mapping from word to index**\n",
    "\n",
    "- **Tokenization**\n",
    "    - Similar to the string split function .split().\n",
    "- Few thing to consider:\n",
    "    - **Punctuation** may be important for downstream NLP tasks (eg: \"I hate cats.\" VS. \"I hate cats?). Depend on the objective ot results whether punctuation is important. Note, higher dimension is required if not considered (eg. \"cats.\" and \"cats?\" is 2 different token.\n",
    "    - SKLearn CountVectorizer ignote punctuatuion.\n",
    "    -**Casing**: consider sentiment analysis or spam detection. Eg.: does \"cat\" have the same meaning as \"Cat\"\n",
    "        - use string.lower() function in python or CountVectorizer(lowercase=True) is SKLearn. \n",
    "    -**Accents**: less common in english\n",
    "        - use CountVectorizer(strip_accents=True)\n",
    "    - **Word-based tokenization**: word is more meaning full than character (eg:cats, dogs, etc). However, when there are too many words, it can take alot of spaces and using DL model accuracy may take an effeect since every word generate probabilistic distribution (too much).\n",
    "    - **Character based tokenization**: character only have 26 letters, and characters represent whitespaces and punctuation. which does not take alot of space and easy to represent in a computer. However, it is less meaningfull than word base.\n",
    "    -**Subword-Based Tokenization**: Middle ground between word-based and character-based. Eg Walking -> \"walk\" + \"ing\". Walk is close related to walking and can be represent as the token \"walk\". While \"ing\" can be applied to \"walking\", \"eating\". However, more data is needed to let the model know that \"walk\" and \"walking\" are similar.\n",
    "    - SKLearn tokenization -> word-based  = CountVectorizer(analyzer=\"word\") (**default**), character-based = CountVectorizer(analyzer=\"char\")\n",
    " \n",
    "- **Stopwords**: \n",
    "    - commonly used words that carry very little useful information. Eg: \"a\",\"the\",\"is\",\"and\",\"are\". it is better to ignore to reduce dimension.\n",
    "    - in SKLearn - set CountVectorizer(stop_words=\"english\"/\"list_of_user_defined_words\"). The default is None\n",
    "    - use nltk libary (language includes, english, arabic, germany,..)\n",
    "\n",
    "- **Stemming and Lemmatization**:\n",
    "    - uisng basic word tokenization, each variation of word has its own vector component. Eg:\"walk\", \"walking\", \"walks\",\"walked\".\n",
    "    - this can leads to high dimensional vectors. The solution is to convert words to their root word.\n",
    "    - 2 common approach are stemming (remove the end of the word and can be inaccurate) and Lemmatization (uses actual rules of language and the true root word is returned)\n",
    "    - **Stemming eg**:\n",
    "      ```\n",
    "      from nltk.stem import PorterStemmer\n",
    "      \n",
    "      porter = PorterStemmer()\n",
    "      porter.stem(\"talking\") # returns 'talk'\n",
    "      ```\n",
    "    - **Lemmatization**:\n",
    "      ```\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    from nltk.corpus import wordnet\n",
    "    \n",
    "    nltk.download(\"wordnet\") # only need to do once\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatizer.lemmatize(\"mice\") # returns 'mouse'\n",
    "    lemmatizer.lemmatize(\"going\") # returns 'going'\n",
    "    lemmatizer.lemmatize(\"going\", pos=wordnet.VERB) # returns 'go', pos = \"part of speech\"\n",
    "      ```\n",
    "      - Inorder to properly use the lemmetizer, POS tagging is required\n",
    "    - map tags is required to allow WordNet and NLTK to be compitable.\n",
    "    - stemming/lemmatization are used in search engines, document retrieval, online ads, social media tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a1dd178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('walk', 'walk', 'ran', 'run', 'boss', 'replac')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer # import stemming and lemmatizer\n",
    "# Stemming\n",
    "porter = PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "porter.stem(\"walking\"),porter.stem(\"walked\"),porter.stem(\"ran\"),porter.stem(\"running\"),porter.stem(\"bosses\"),porter.stem(\"replacement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c88c6c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemmat is more sophist than stem "
     ]
    }
   ],
   "source": [
    "sentence = \"Lemmatization is more sophisticated than stemming\".split() # split the sentence\n",
    "for token in  sentence:\n",
    "    print(porter.stem(token), end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "715ab9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\ZAKY-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('walking', 'walk', 'going', 'go', 'mouse', 'wa', 'be', 'good')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lemmmatization \n",
    "nltk.download('omw-1.4')\n",
    "# nltk.download(\"wordnet\")\n",
    "from nltk.corpus import wordnet \n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize(\"walking\"), lemmatizer.lemmatize(\"walking\", pos=wordnet.VERB), lemmatizer.lemmatize(\"going\"), lemmatizer.lemmatize(\"going\", pos=wordnet.VERB), lemmatizer.lemmatize(\"mice\"), lemmatizer.lemmatize(\"was\"), lemmatizer.lemmatize(\"was\", pos=wordnet.VERB), lemmatizer.lemmatize(\"better\", pos=wordnet.ADJ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "220f0bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automate the pos tagging\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0c1113f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ZAKY-PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download tagger\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad566d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Donald', 'NNP'),\n",
       " ('Trump', 'NNP'),\n",
       " ('has', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('devoted', 'VBN'),\n",
       " ('following', 'NN')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"Donald Trump has a devoted following\".split()\n",
    "words_and_tags = nltk.pos_tag(sentence)\n",
    "print(words_and_tags)\n",
    "print('----------------')\n",
    "for word, tag in words_and_tags:\n",
    "    lemma = lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag))\n",
    "    print(lemma, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa7a0e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('cat', 'NN'), ('was', 'VBD'), ('following', 'VBG'), ('the', 'DT'), ('bird', 'NN'), ('as', 'IN'), ('it', 'PRP'), ('flew', 'VBD'), ('by', 'IN')]\n",
      "----------------\n",
      "the cat be follow the bird a it fly by "
     ]
    }
   ],
   "source": [
    "sentence_2 = \"the cat was following the bird as it flew by\".split()\n",
    "words_and_tags = nltk.pos_tag(sentence_2)\n",
    "print(words_and_tags)\n",
    "print('----------------')\n",
    "for word, tag in words_and_tags:\n",
    "    lemma = lemmatizer.lemmatize(word, pos=get_wordnet_pos(tag))\n",
    "    print(lemma, end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b4730b",
   "metadata": {},
   "source": [
    "## 15. Vector Similarity\n",
    "- Finding similarity between words\n",
    "- Application: Article Spinning: rewrite existing articles by replacing every few similar words.\n",
    "\n",
    "We assume vectors in ML live in euclidean space. so each component is the length of the vector in that respective direction. Therefore, we can use the **distance** to define the **similarity** (the further the distance, the less similar).\n",
    "\n",
    "- Finding vector similarity:\n",
    "    - **Euclidean Distance**:\n",
    "    $$||x-y||_2 = \\sqrt{(x_1 - y_1)^2 + (x_2 - y_2)^2 + ... + (x_D - y_D)^2} $$\n",
    "     - Take 2 vectors of *x* and *y* each with size D and find the difference of each component and stuare the difference and add them together and take the square root of the result.\n",
    "\n",
    "    - Angle between 2 vectors (**cosine of the angle**)\n",
    "        - relationship between cosine distance and cosine similarity can be represent as: \n",
    "          **Cosine distance = 1- Cosine Similarity**\n",
    "        $$\n",
    "        dist = 1 - sim \\\\\n",
    "        dist = 1 - 1 \\\\\n",
    "        dist = 0 \n",
    "        $$\n",
    "        - Therefore, when it is very similar (1), the distance is 0 (eg: parallel). Where else, if the two vectors are anti-parallel, the cosine similarity become -1. Thus the cosine distance is 2 (maximum possible distance)\n",
    "        - Cosine distance is not a true distance metric as it does not satisfy the triangle inequality\n",
    "        \n",
    "    - Which one to use?\n",
    "        - when vectors of different sizes cosine distance might not be useful/accurate\n",
    "    - When are cosine distance and euclidean distance equivalent?\n",
    "        - when you normalize your vectors and only task is to rank items, then it doesnt matter which similarity or  distance to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9a53f0",
   "metadata": {},
   "source": [
    "##  18. Term Frequency – Inverse Document Frequency (TF-IDF)\n",
    " - method for count vectorizer\n",
    " - commonly used for documents retrieval and text mining\n",
    " - what's wrong with count vectorizer?\n",
    "     - Stopwords is not useful for NLP tasks since it is in every documents/ articles\n",
    "     - How do we know our list of stopword is correct? -> We do not! since stop words can be application specific\n",
    " - **TF-IDF**:\n",
    " $$\n",
    "     TF - IDF = \\frac{Term Frequency}{Document Frequency}\\\n",
    " $$\n",
    "         - Term Frequency: the number of count it appears\n",
    "         - Document Frequency: the number of document the word appears \n",
    "     - Document frequency increase when the word apear more in more documents\n",
    "     - Common TF_IDF variation:\n",
    "     $$\n",
    "     tfidf(t,d) = tf(t,d) * idf(t)\n",
    "     $$\n",
    "         - where t is term and d is document\n",
    "       - IDF:\n",
    "       $$\n",
    "       idf(t) = log \\frac{N}{N(t)}\n",
    "       $$\n",
    "         - Where N is total number documents and thus N(t) the number of documents term t appears in.\n",
    "         - log function is used to reduce its arguments (lets say 1000000 documents, using log can reduce the number without alternate the result/meaning)\n",
    "     - TF-IDF in python\n",
    "     ```\n",
    "     from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "     tfidf = TfidfVectorizer()\n",
    "     x_train = tfidf.fit_transform(train_texts)\n",
    "     x_test = tfidf.transform(test_texts)\n",
    "     \n",
    "     # note: arguments exist for stopwords, tokenizer, strip accents, etc\n",
    "     ```\n",
    "     \n",
    "     - **TF variations**:\n",
    "         - Binary (1 if word appears, 0 otherwise)\n",
    "         - Normalize the count\n",
    "         $$\n",
    "         tf(t,d) = \\frac{count(t,d)}{ \\sum \\limits _{t'\\in terms(d)}count(t',d)}\n",
    "         $$\n",
    "         - Take the log\n",
    "         $$ tf(t,d) = log(1+count(t,d)) $$\n",
    "     \n",
    "     - **IDF variation**:\n",
    "         - Smooth IDF: preventing IDF = 0\n",
    "         $$ idf(t) = log \\frac{N}{N(t)+1} +1 $$\n",
    "         - IDF Max: using maximum term count from the same document and thus the ratio instide the log is relative to  the current document instead of the whole dataset.\n",
    "         $$ idf(t) = log \\frac{ \\underset{t'\\in terms{d}}{max}N{t'}}{N(t)} $$\n",
    "         - probabilistic IDF\n",
    "         $$ idf(t) = log \\frac{N - N(t)}{N(t)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b31bae0",
   "metadata": {},
   "source": [
    "## 19. Word-to-Index Mapping\n",
    "- When converting documents to vectors, the result is a **document-term matrix** \n",
    "- Row = document, column = term (size = #documents x #terms)\n",
    "- Which column corresponds to which word?\n",
    "  `` \n",
    "  current_idx = 0 # initalize a variable called current index  to 0\n",
    "  word2idx = {} # empty dictionary\n",
    "  for doc in documents: # loop through documents\n",
    "  tokens = word_tokenize(doc) # tokenize the documents\n",
    "  for token in tokens:\n",
    "      if token not in word2idx: # Check if token exist in word2idx dictionary\n",
    "          word2idx[token] = current_idx # if does not exist, dictionary ss thecurent token and the corresponding values become the index\n",
    "          current_idx += 1 # increment the index\n",
    "   ```\n",
    "- Word-to-index mapping -> Count Vectorizer -> TF-IDF\n",
    "- What to do with words in the  test set but but in train set?\n",
    "    1. Ignore those words\n",
    "    2. Create special index for unknown/rare words\n",
    "- Reverse mapping (index-to-eord) is necessary to allow us know which features are reffered to when the output is the index (therefore index to word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d7c16f",
   "metadata": {},
   "source": [
    "## 21. Neural Word Embeddings\n",
    "- method to create vector out of text\n",
    "- word embedding is similar to word vectors\n",
    "- typically use to convert words into vectors instead of documents. thus a document become sequence of vectors\n",
    "- more information than bag of words (the order is considered unlike bag of words)\n",
    "- 2 common method: word2vex and Glove\n",
    "    - **Word2vec** (feed forward Neural Network)\n",
    "        - Embeddings are stored in the weight of the NN\n",
    "        - Need to find the weight of the NN, which effectively are the word embedding themselves\n",
    "        - given an input word, it predict whhether an output word appears in its context\n",
    "        - eg: given a sentence of \"The quick brown fox jumps over the lazy dogs\" and the input is jumps, the correct output (in binary) should be words that are close to the word \"jumps\"\n",
    "    - **Glove**\n",
    "        - does not use ANN primarly. However, at later stages, it uses\n",
    "        - similar to recommender system\n",
    "        - look for patterns in which users liked which movies (eg: if Bob and Alice rate movies similarly, then  they can be sued to estimate each other's unknown ratings)\n",
    "        - eg: given a sentence of \"The quick brown fox jumps over the lazy dogs\" and the input is jumps, it score fox to 1/1 (closest) and brown 1/2 (further). the further the word is the lower score it gets.\n",
    "        - thus raing is based on the distance between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b7c58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
